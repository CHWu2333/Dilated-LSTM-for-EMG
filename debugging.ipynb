{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymatreader import read_mat\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn import preprocessing\n",
    "import argparse\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    window_size = 600\n",
    "    step_size = 20\n",
    "    batch_size = 128\n",
    "    dilation = 0\n",
    "    num_layers = 3\n",
    "    z_score_norm = True\n",
    "    hidden_size = 12\n",
    "    input_size = 12\n",
    "    num_kernels = (32, 64, 64, 128, 128, 256, 256)\n",
    "    list_dilation = (1, 2, 4, 8, 8, 8, 1)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    factor = 0.5\n",
    "    patience = 50\n",
    "    threshold = 1e-2\n",
    "    lr_limit = 1e-4\n",
    "    measurement = 'min'\n",
    "    scheduler = 'plateau'\n",
    "    optimizer = 'adam'\n",
    "    weight_decay = 1e-2\n",
    "    progress_bar = True\n",
    "    cuda = True\n",
    "    test = False\n",
    "    plot = False\n",
    "    save = False\n",
    "    scheduler = 'plateau'\n",
    "    weight_decay = 1e-2\n",
    "\n",
    "    training_set = (1, 2)\n",
    "    testing_set = (3, 4)\n",
    "    epoch = 20\n",
    "    cuda = cuda and torch.cuda.is_available()\n",
    "    weight_decay = float(weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Model, self).__init__()\n",
    "        # single LSTM\n",
    "        self.window_size = args.window_size\n",
    "        self.num_layers = args.num_layers\n",
    "        self.input_size = args.input_size\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.dilation = args.dilation\n",
    "        self.dilated_n_steps = self.window_size // (self.dilation + 1)\n",
    "        self.lstm1 = nn.LSTM(input_size=12, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.lstm4 = nn.LSTM(input_size=128, hidden_size=128, num_layers=1, batch_first=True)\n",
    "\n",
    "        # CNN Blocks\n",
    "        # num_kernels = (32, 64, 64, 128, 128, 256, 256)\n",
    "        # list_dilation = (1, 2, 4, 8, 8, 8, 1)\n",
    "        self.conv1 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=5, dilation=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, dilation=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=5, dilation=1)\n",
    "        self.conv4 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=5, dilation=1)\n",
    "        self.conv5 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, dilation=1)\n",
    "        self.conv6 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=5, dilation=1)\n",
    "        self.conv7 = nn.Conv1d(in_channels=256, out_channels=256, kernel_size=5, dilation=1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features=32)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(num_features=64)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(num_features=64)\n",
    "        self.batch_norm4 = nn.BatchNorm1d(num_features=128)\n",
    "        self.batch_norm5 = nn.BatchNorm1d(num_features=128)\n",
    "        self.batch_norm6 = nn.BatchNorm1d(num_features=256)\n",
    "        self.batch_norm7 = nn.BatchNorm1d(num_features=256)\n",
    "        self.PReLU1 = nn.PReLU()\n",
    "        self.PReLU2 = nn.PReLU()\n",
    "        self.PReLU3 = nn.PReLU()\n",
    "        self.PReLU4 = nn.PReLU()\n",
    "        self.PReLU5 = nn.PReLU()\n",
    "        self.PReLU6 = nn.PReLU()\n",
    "        self.PReLU7 = nn.PReLU()\n",
    "\n",
    "        # Full connection blocks\n",
    "        self.Flatten = nn.Flatten(1, 2)\n",
    "        self.Dense1 = nn.Linear(146432, 64)\n",
    "        self.Dense2 = nn.Linear(64, 32)\n",
    "        self.Dense3 = nn.Linear(32, 17)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(np.shape(x))\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        np.shape(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.PReLU1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.PReLU2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.PReLU3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.PReLU4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.batch_norm5(x)\n",
    "        x = self.PReLU5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.batch_norm6(x)\n",
    "        x = self.PReLU6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.batch_norm7(x)\n",
    "        x = self.PReLU7(x)\n",
    "        \n",
    "#         print('after conv:',x.size())\n",
    "\n",
    "        x = self.Flatten(x)\n",
    "#         print('after flat:', x.size())\n",
    "        x = self.Dense1(x)\n",
    "#         print('after dense1', x.size())\n",
    "        x = self.Dense2(x)\n",
    "        x = self.Dense3(x)\n",
    "        out = torch.argmax(x, dim=1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(args, data_set):\n",
    "    \"\"\"\n",
    "    This function is called when loading the data, it takes a lot of time.\n",
    "    The function is will turn all E1_A1 data to a (x, window_size, 2 + channel_num)\n",
    "    In which, x is the number of all pieces of windowing,\n",
    "    window size in paper is set to be 600, which is 300ms\n",
    "    step_size in paper is set to be 20 which is 10 ms\n",
    "    channel_num in dataset is 12, fixed.\n",
    "    Outputs:\n",
    "        Input: EMG data with 12 channels, shape:(x, window_size, channel_num)\n",
    "        label: EMG corresponding restimulus digit, shape:(x,)\n",
    "    \"\"\"\n",
    "    data_dir = '/scratch/cw3755/my_project/data_E1/'\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in data_set:\n",
    "        f = data_dir + '/S' + str(i) + '_E1_A1.mat'\n",
    "        data_raw = read_mat(f)\n",
    "        if args.z_score_norm:\n",
    "            emg = preprocessing.scale(data_raw['emg'])\n",
    "        df1 = pd.DataFrame(emg)\n",
    "        df2 = pd.DataFrame(data_raw['restimulus'])\n",
    "        df3 = pd.DataFrame(data_raw['repetition'])\n",
    "        df = pd.concat([df3, df2, df1], axis=1)\n",
    "        for repetition in range(1, 7):\n",
    "            for restimulus in range(1, 18):\n",
    "                df4 = df.loc[df.iloc[:, 0] == repetition, :]\n",
    "                df5 = df4.loc[df.iloc[:, 0] == restimulus, :]\n",
    "                for step in range((df5.shape[0] - args.window_size) // args.step_size):\n",
    "                    inputs.append(\n",
    "                        df5.iloc[args.step_size * step:args.step_size * step + args.window_size, 2:].values)\n",
    "                    labels.append(restimulus)\n",
    "    inputs = torch.FloatTensor(np.array(inputs))\n",
    "    labels = torch.FloatTensor(np.array(labels))\n",
    "#     print(inputs.size())\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    inputs = inputs.type(torch.cuda.FloatTensor)\n",
    "    labels = labels.cuda()\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat Net Work\n",
    "net = Model(args)\n",
    "if args.cuda:\n",
    "    net = net.cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate, amsgrad=True,\n",
    "                                   weight_decay=1e-5 * args.batch_size)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, args.measurement, threshold=args.threshold,factor=args.factor,patience=args.patience, min_lr=args.lr_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "Data loaded! Time used: 80.67961049079895\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "print('loading data...')\n",
    "t1 = time.time()\n",
    "train_data, train_label = data_preprocessing(args, args.training_set)\n",
    "test_data, test_label = data_preprocessing(args, args.testing_set)\n",
    "# print(np.shape(self.train_data))\n",
    "# print(np.shape(self.train_label))\n",
    "train = TensorDataset(train_data, train_label)\n",
    "test = TensorDataset(test_data, test_label)\n",
    "#train = [[train_data[i, :, :], train_label[i]] for i in range(len(train_label))]\n",
    "#test = [[test_data[i, :, :], test_label[i]] for i in range(len(test_label))]\n",
    "# print(np.shape(train))\n",
    "# print(np.shape(test))\n",
    "\n",
    "# train_data = train_data.clone().detach().requires_grad_(True)\n",
    "# test_data = test_data.clone().detach().requires_grad_(True)\n",
    "# train_label = train_label.clone().detach().requires_grad_(True)\n",
    "# test_label = test_label.clone().detach().requires_grad_(True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=args.batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, shuffle=False, batch_size=args.batch_size)\n",
    "t2 = time.time()\n",
    "print('Data loaded! Time used:', t2 - t1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#new training model\n",
    "dataset = TensorDataset(train_data, train_label)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "model = Model(args).cuda()\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "dataset_size = len(dataloader.dataset)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for id_batch,(x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        x_batch = x_batch.requires_grad_()\n",
    "        y_batch = y_batch.requires_grad_()\n",
    "\n",
    "        y_batch_pred = model(x_batch)\n",
    "        y_batch_pred = y_batch_pred.type(torch.float32)\n",
    "\n",
    "        \n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "        #optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if id_batch %10 == 0:\n",
    "            loss,current = loss.item(), (id_batch+1)*len(x_batch)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/878 [06:13<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/874 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3674.291748  [  128/112378]\n",
      "tensor([ 7., 13.,  1., 12.,  0.,  5., 12.,  5.,  5., 13., 12., 13.,  4.,  5.,\n",
      "        12., 13., 13.,  5., 13.,  3.,  0.,  5.,  9.,  8.,  5.,  5.,  0., 13.,\n",
      "         5.,  4.,  5., 13., 13.,  5.,  7.,  5., 13.,  0., 13., 12.,  0., 13.,\n",
      "         7., 13., 13.,  8.,  7.,  5., 10., 13., 13.,  7.,  5.,  5.,  5., 12.,\n",
      "        10., 12., 10.,  0., 13.,  5.,  5.,  5.,  5., 13.,  5.,  5.,  5.,  0.,\n",
      "        13., 13., 13., 13.,  3.,  5., 13.,  7., 13., 13., 13.,  5., 13., 13.,\n",
      "         5.,  5., 12.,  5., 13.,  5., 12., 13., 13., 13.,  5.,  5.,  5.,  0.,\n",
      "         5.,  5., 13., 13., 13., 13., 13.,  5., 13.,  5.,  5.,  3., 12., 12.,\n",
      "        12., 13.,  5., 13., 13.,  5., 10.,  5.,  5., 12.,  7.,  5.,  0., 13.,\n",
      "         5.,  5.], device='cuda:0')\n",
      "tensor([6., 1., 5., 2., 3., 5., 2., 2., 4., 6., 4., 6., 6., 1., 3., 2., 6., 6.,\n",
      "        2., 4., 4., 4., 2., 2., 6., 2., 6., 2., 5., 3., 2., 2., 6., 5., 3., 3.,\n",
      "        6., 4., 3., 4., 3., 5., 6., 4., 4., 2., 2., 4., 5., 3., 5., 4., 2., 6.,\n",
      "        3., 4., 4., 2., 3., 3., 3., 2., 5., 3., 6., 1., 2., 4., 2., 2., 1., 4.,\n",
      "        1., 1., 4., 4., 4., 4., 3., 4., 2., 2., 2., 3., 4., 5., 4., 4., 4., 2.,\n",
      "        6., 6., 4., 4., 3., 3., 1., 2., 6., 3., 2., 2., 5., 2., 5., 2., 3., 2.,\n",
      "        1., 1., 3., 4., 3., 4., 6., 3., 3., 6., 1., 3., 6., 4., 5., 5., 6., 2.,\n",
      "        3., 2.], device='cuda:0', requires_grad=True)\n",
      "loss: 3632.357422  [ 1408/112378]\n",
      "loss: 3683.902344  [ 2688/112378]\n",
      "loss: 3917.852539  [ 3968/112378]\n",
      "loss: 3765.190430  [ 5248/112378]\n",
      "loss: 3861.598389  [ 6528/112378]\n",
      "loss: 3790.215820  [ 7808/112378]\n",
      "loss: 3842.065918  [ 9088/112378]\n",
      "loss: 3452.769531  [10368/112378]\n",
      "loss: 3778.026611  [11648/112378]\n",
      "loss: 3595.717285  [12928/112378]\n",
      "loss: 3919.361328  [14208/112378]\n",
      "loss: 3479.889893  [15488/112378]\n",
      "loss: 3914.082520  [16768/112378]\n",
      "loss: 3605.264648  [18048/112378]\n",
      "loss: 3832.735840  [19328/112378]\n",
      "loss: 3842.556885  [20608/112378]\n",
      "loss: 3693.674561  [21888/112378]\n",
      "loss: 4099.709961  [23168/112378]\n",
      "loss: 3506.413086  [24448/112378]\n",
      "loss: 3595.509033  [25728/112378]\n",
      "loss: 3872.777832  [27008/112378]\n",
      "loss: 3719.184082  [28288/112378]\n",
      "loss: 3927.343262  [29568/112378]\n",
      "loss: 3941.024902  [30848/112378]\n",
      "loss: 3797.148926  [32128/112378]\n",
      "loss: 3964.078125  [33408/112378]\n",
      "loss: 3667.902344  [34688/112378]\n",
      "loss: 3490.749512  [35968/112378]\n",
      "loss: 3809.107910  [37248/112378]\n",
      "loss: 3943.358643  [38528/112378]\n",
      "loss: 4052.364746  [39808/112378]\n",
      "loss: 3733.030273  [41088/112378]\n",
      "loss: 3255.759033  [42368/112378]\n",
      "loss: 3833.289062  [43648/112378]\n",
      "loss: 3677.534180  [44928/112378]\n",
      "loss: 3589.792969  [46208/112378]\n",
      "loss: 3592.123779  [47488/112378]\n",
      "loss: 4027.335205  [48768/112378]\n",
      "loss: 3971.515869  [50048/112378]\n",
      "loss: 3651.260254  [51328/112378]\n",
      "loss: 3431.781006  [52608/112378]\n",
      "loss: 4250.049805  [53888/112378]\n",
      "loss: 3546.195068  [55168/112378]\n",
      "loss: 4096.790527  [56448/112378]\n",
      "loss: 3909.125488  [57728/112378]\n",
      "loss: 3831.535400  [59008/112378]\n",
      "loss: 3502.599609  [60288/112378]\n",
      "loss: 3779.664307  [61568/112378]\n",
      "loss: 3765.517090  [62848/112378]\n",
      "loss: 3694.339111  [64128/112378]\n",
      "loss: 3851.357422  [65408/112378]\n",
      "loss: 3654.283691  [66688/112378]\n",
      "loss: 3552.432129  [67968/112378]\n",
      "loss: 3210.277832  [69248/112378]\n",
      "loss: 3669.538574  [70528/112378]\n",
      "loss: 3394.845703  [71808/112378]\n",
      "loss: 4082.297607  [73088/112378]\n",
      "loss: 3713.667969  [74368/112378]\n",
      "loss: 3514.519043  [75648/112378]\n",
      "loss: 3411.704834  [76928/112378]\n",
      "loss: 3408.778809  [78208/112378]\n",
      "loss: 3707.497070  [79488/112378]\n",
      "loss: 3841.997070  [80768/112378]\n",
      "loss: 4015.493652  [82048/112378]\n",
      "loss: 3660.010742  [83328/112378]\n",
      "loss: 3347.752441  [84608/112378]\n",
      "loss: 3768.022461  [85888/112378]\n",
      "loss: 3686.310303  [87168/112378]\n",
      "loss: 3598.265625  [88448/112378]\n",
      "loss: 3679.950684  [89728/112378]\n",
      "loss: 4066.690674  [91008/112378]\n",
      "loss: 3660.427734  [92288/112378]\n",
      "loss: 3490.486572  [93568/112378]\n",
      "loss: 3628.351807  [94848/112378]\n",
      "loss: 3655.277588  [96128/112378]\n",
      "loss: 3846.470703  [97408/112378]\n",
      "loss: 3672.793945  [98688/112378]\n",
      "loss: 3519.253418  [99968/112378]\n",
      "loss: 3827.444824  [101248/112378]\n",
      "loss: 3626.036621  [102528/112378]\n",
      "loss: 3284.704102  [103808/112378]\n",
      "loss: 3573.461914  [105088/112378]\n",
      "loss: 3900.006348  [106368/112378]\n",
      "loss: 3780.751709  [107648/112378]\n",
      "loss: 3703.832031  [108928/112378]\n",
      "loss: 3434.236816  [110208/112378]\n",
      "loss: 3638.920898  [111488/112378]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/874 [01:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"host_softmax\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4207849a74b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0my_batch_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtest_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"host_softmax\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "lr_list = []\n",
    "\n",
    "train_dataset = TensorDataset(train_data, train_label)\n",
    "test_dataset = TensorDataset(test_data, test_label)\n",
    "trainloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False) \n",
    "model = Model(args).cuda()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, amsgrad=True, weight_decay=1e-5 * args.batch_size)\n",
    "dataset_size = len(trainloader.dataset)\n",
    "print(dataset_size)\n",
    "\n",
    "for e in range(1, args.epoch+1):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    if args.progress_bar:\n",
    "        trainloder = tqdm.tqdm(trainloader)\n",
    "        testloader = tqdm.tqdm(testloader)\n",
    "    optimizer.zero_grad()\n",
    "    for id_batch,(x_batch, y_batch) in enumerate(trainloader):\n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        x_batch = x_batch.requires_grad_()\n",
    "        y_batch = y_batch.requires_grad_()\n",
    "        y_batch_pred = model(x_batch)\n",
    "        y_batch_pred = y_batch_pred.type(torch.float32)\n",
    "        if id_batch == 2:\n",
    "            print(y_batch_pred)\n",
    "            print(y_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if id_batch %10 == 0:\n",
    "            loss,current = loss.item(), (id_batch+1)*len(x_batch)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{dataset_size:>5d}]\")\n",
    "    if args.test:\n",
    "        break\n",
    "\n",
    "\n",
    "    for id_batch,(x_batch, y_batch) in enumerate(testloader):\n",
    "        with torch.no_grad():\n",
    "            x_batch = x_batch.cuda()\n",
    "            y_batch = y_batch.cuda()\n",
    "            x_batch = x_batch.requires_grad_()\n",
    "            y_batch = y_batch.requires_grad_()\n",
    "            y_batch_pred = model(x_batch)\n",
    "            test_acc += sum(i == j for i,j in zip(y_batch_pred, y_batch))\n",
    "            loss = loss_fn(y_batch_pred, y_batch)\n",
    "            test_loss += loss.item()\n",
    "        if args.test:\n",
    "            break\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    test_loss = test_loss / len(testloader)\n",
    "    test_acc = test_acc / 10000\n",
    "    test_acc = test_acc.tolist()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "    test_acc.append(test_acc)\n",
    "    lr_list.append(lr)\n",
    "    if args.scheduler == 'plateau':\n",
    "        if args.measurement == 'min':\n",
    "            scheduler.step(test_loss)\n",
    "        else:\n",
    "            scheduler.step(test_acc)\n",
    "    print(f'epoch {epoch}, train loss {train_loss:.4}, test loss {test_loss:.4}, test acc {test_acc:.4}, lr {lr:.4}')\n",
    "if args.save:\n",
    "    torch.save(model.state_dict(), f\"{int(time.time())}.pt\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#train model\n",
    "print(args.cuda)\n",
    "for epoch in range(1, args.epoch + 1):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    if args.progress_bar:\n",
    "        train_loader = tqdm.tqdm(train_loader)\n",
    "        test_loader = tqdm.tqdm(test_loader)\n",
    "\n",
    "        inputs = train_data.cuda()\n",
    "        labels = train_label.cuda()\n",
    "        if args.cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        # print('inputs shape = ', np.shape(inputs))\n",
    "        predicted_output = net(inputs)\n",
    "        fit = loss(predicted_output, labels)\n",
    "        fit.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += fit.item()\n",
    "        if args.test:\n",
    "            break\n",
    "    for i, data in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = data\n",
    "            if args.cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            predicted_output = net(inputs)\n",
    "            test_acc += sum(i == j for i, j in zip(torch.argmax(predicted_output, dim=1), labels))\n",
    "            fit = loss(predicted_output, labels)\n",
    "            test_loss += fit.item()\n",
    "        if args.test:\n",
    "            break\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = test_acc / 10000\n",
    "    test_acc = test_acc.tolist()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    train_loss.append(train_loss)\n",
    "    test_loss.append(test_loss)\n",
    "    test_acc.append(test_acc)\n",
    "    lr.append(lr)\n",
    "    if args.scheduler == 'plateau':\n",
    "        if args.measurement == 'min':\n",
    "            scheduler.step(test_loss)\n",
    "        else:\n",
    "            scheduler.step(test_acc)\n",
    "\n",
    "    print(\n",
    "        f'epoch {epoch}, train loss {train_loss:.4}, test loss {test_loss:.4}, test acc {test_acc:.4}, lr {lr:.4}')\n",
    "\n",
    "if args.save:\n",
    "    torch.save(net.state_dict(), f\"{int(time.time())}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
